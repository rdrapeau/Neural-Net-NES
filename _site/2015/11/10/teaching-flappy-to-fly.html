<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Teaching Flappy to Fly</title>
  <meta name="description" content="Our initial goal for the midpoint milestone was to have learning implemented on the simplest NES game we could find, which was lifeforce. However, this game ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/2015/11/10/teaching-flappy-to-fly.html">
  <link rel="alternate" type="application/rss+xml" title="Deep Q Learning" href="/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Deep Q Learning</a>

    <img class="bird" src="/assets/bird.png" />
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
        
          
        
          
          <a class="page-link" href="/milestones/">Project Milestones</a>
          
        
          
          <a class="page-link" href="/posts.html">Blog</a>
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Teaching Flappy to Fly</h1>
    <p class="post-meta"><time datetime="2015-11-10T13:44:40-08:00" itemprop="datePublished">Nov 10, 2015</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Our initial goal for the midpoint milestone was to have learning implemented on the simplest NES game we could find, which was lifeforce. However, this game has quite a lot of complexity. The action space is 5 dimensional (forward, back, up, down, shoot), and the feature space would be the pixels of the game itself. We decided to start with flappy bird, because it only has 2 possible actions and we can extract much simpler features.</p>

<p>We are using the deep q learning implementation in the <a href="http://cs.stanford.edu/people/karpathy/convnetjs/">ConvNetJS library</a> to train our bird to fly. The ConvNetJS q learning algorithm is based off of the deep q learning with experience replay algorithm described in the paper <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>.</p>

<p>Training an agent to learn flappy bird sounds simple, but we faced some challenges. At first we tried to use an existing javascript implementation of flappy bird, but it was too slow and did not fit nicely with how we wanted to hook up the brain and the game. The game was written such that it was tied to the DOM (the bird, pipes, etc were DOM elements). Thus collision detection was literal testing of overlapping DOM elements, and it was impossible to turn off rendering because the DOM is inherently rendered. We could overclock the rendering but anything faster than 80 FPS was unreliable. Rewriting the game gave us a much cleaner version of flappy bird, eliminating unnecessary frills and including an option not to render the game to speed things up during training. </p>

<p>The first task was to teach the agent to always flap. We tested the deep q learning algorithm on a model that rewarded an action of 1 and penalized an action of 0, and found that the results quickly converged and the agent learned to choose an action 1 over 0. This is exactly the behavior we desired from flappy. However, when we tried to carry this reward model over to flappy bird, our average Q learning loss quickly went from a very large number to infinity, and then to NaN. We eventually determined that the cause was the large numbers in our state space. They were causing the neural network to spiral out of control. Once we normalized our state space values, our bird was able to learn.  </p>

<p><img class="" src="/assets/learning.gif" width="60%" />  </p>

<p><strong>Fig. 1: Q learning stats during training</strong>  </p>

<p><img class="" src="/assets/flappy-flying.gif" />  </p>

<p><strong>Fig. 2: After we trained our bird to always fly</strong>  </p>

<p>Our next step is experimenting with various reward models and state inputs to teach our agent to beat flappy bird.</p>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Deep Q Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
	  <li><a href="https://github.com/rdrapeau">Ryan Drapeau</a></li>
	  <li><a href="https://github.com/sonjakhan">Sonja Khan</a></li>
	  <li><a href="https://github.com/aaronnech">Aaron Nech</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/rdrapeau/neural-net-nes"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">neural-net-nes</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Project site and blog for CSE571
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
